{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage d'un modèle de segmentation d'images\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans cette partie, nous allons mettre en oeuvre un modèle de segmentation d'images sur un dataset particulier. Contrairement à la partie sur les modèles fondations, nous allons désormais effectuer l'ensemble de l'apprentissage ainsi que la prédiction, et l'évaluation du modèle.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Mettre en oeuvre un modèle de segmentation d'images\n",
    "- Comprendre les spécificités de l'apprentissage de modèles de segmentation\n",
    "- Comprendre les métriques d'évaluation des modèles de segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les Données\n",
    "\n",
    "Pour pouvoir effectuer l'apprentissage d'un modèle, il faut avoir accès à un jeu de données étiquetés. Pour ce faire, nous allons utiliser le [dataset emps](https://github.com/by256/emps) composé de 465 images de microscope. \n",
    "\n",
    "![emps.png](./assets/images/emps.png)  \n",
    "\n",
    "Les images, ainsi que les segmentations effectuées sont disponibles dans le répertoire `data/emps`. Les images sont stockées dans le répertoire `data/emps/images` et les masques de segmentation dans le répertoire `data/emps/masks`. Les images sont au format `png` et les masques de segmentation sont au format `npy`.\n",
    "\n",
    "Les masques de segmentation identifie non seulement le premier plan de l'arrière plan, mais également les différentes régions. Chaque région est identifiée par un entier différent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des images et des masques\n",
    "import os\n",
    "input_dir = ... # Chemin vers le répertoire contenant les images\n",
    "target_dir = ... # Chemin vers le répertoire contenant les masques\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [os.path.join(input_dir, fname) for fname in os.listdir(input_dir) if fname.endswith(\".png\")]\n",
    ")\n",
    "\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname) for fname in os.listdir(target_dir) if fname.endswith(\".png\") and not fname.startswith(\".\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des images et des masques\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "idx_image = ... #index de l'image à afficher \n",
    "\n",
    "img = img_to_array(load_img(input_img_paths[idx_image], color_mode=\"grayscale\"))\n",
    "mask = img_to_array(load_img(target_paths[idx_image], color_mode=\"grayscale\"))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(... ) # affichage de l'image\n",
    "axes[1].imshow(...) # affichage du masque\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "Afin de pouvoir à la fois apprendre et évaluer notre modèle, nous allons devoir séparer notre jeu de données en deux parties : une partie pour l'apprentissage et une partie pour l'évaluation. Pour ce faire, nous pouvons utiliser la fonction `train_test_split` de la librairie `scikit-learn`. \n",
    "\n",
    "Afin que notre modèle puisse apprendre, nous allons devoir effectuer un certain nombre de transformations sur les images. En effet, les images sont de taille différentes, et il est nécessaire de les redimensionner pour que le modèle puisse les traiter. De plus, il est nécessaire de normaliser les images pour que les valeurs des pixels soient comprises entre 0 et 1. Concernant les masques, nous allons simplement nous focaliser sur la séparation du premier plan de l'arrière plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# definition de la taille des images\n",
    "img_size = (128,128)\n",
    "# le nombre d'images au total\n",
    "nums_imgs = len(input_img_paths)\n",
    "\n",
    "def path_to_input_image(path):\n",
    "    \"\"\"Chargement d'une image et redimensionnement en 128x128\"\"\"\n",
    "    return img_to_array(load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "\n",
    "def path_to_target(path):\n",
    "    \"\"\"Chargement d'un masque et redimensionnement en 128x128. Tous les premiers plan sont affectés à la même valeur\n",
    "    \"\"\"\n",
    "    img = img_to_array(load_img(path, \n",
    "                                target_size=img_size, \n",
    "                                color_mode=\"grayscale\"))\n",
    "    img[img > 0] = 1\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# initalisation des images et des masques\n",
    "input_imgs = np.zeros((nums_imgs,) + img_size +(1,), dtype=\"float32\")\n",
    "masks = np.zeros((nums_imgs,) + img_size +(1,) , dtype=\"uint8\")\n",
    "\n",
    "# conversion des images et des masques\n",
    "for i in range(nums_imgs):\n",
    "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
    "    masks[i] = path_to_target(target_paths[i])\n",
    "\n",
    "# division des données en données d'entrainement et de validation\n",
    "# Splitter les données pour avoir 100 images de validation\n",
    "# documentation sur la fonction train_test_split : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "...  = train_test_split( ... )\n",
    "\n",
    "\n",
    "print(f\"La taille de l'ensemble d'entrainement est {len(train_input_imgs)}\")\n",
    "print(f\"La taille de l'ensemble de validation est {len(val_input_imgs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les images sont en 128x128\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "img = input_imgs[idx_image]\n",
    "mask = masks[idx_image]\n",
    "axes[0].imshow(img, cmap=\"gray\")\n",
    "axes[1].imshow(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en oeuvre du modèle\n",
    "\n",
    "Maintenant que les données sont prêtes, nous allons pouvoir mettre en oeuvre notre modèle. Pour ce faire, nous allons utiliser un modèle de segmentation basé sur un CNN. Plus précisément, nous allons utiliser un modèle U-Net.\n",
    "\n",
    "![ae.svg](./assets/images/ae.svg)\n",
    "\n",
    "Le modèle U-Net est un modèle de segmentation d'images très populaire. Il est composé de deux parties : une partie de contraction et une partie d'expansion. La partie de contraction est composée de couches de convolution et de pooling, tandis que la partie d'expansion est composée de couches de convolution et de upsampling.\n",
    "\n",
    "\n",
    "Pour l'implémentation, nous allons utiliser la librairie `tensorflow` et plus particulièrement le module `keras`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "  \n",
    "def get_model(img_size):\n",
    "    inputs = keras.Input(shape=img_size + (1,))\n",
    "    x = layers.Rescaling(1./255)(inputs)\n",
    "    \n",
    "    # N'hésitez pas à modifier l'architecture du modèle pour observer le changement du nombre de paramètres,\n",
    "    #  le changment de la qualité des prédictions et le temps d'entrainement\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)  \n",
    "   \n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    \n",
    "    outputs =  layers.Conv2D(1, 1, activation='sigmoid', padding=\"same\")(x) \n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "  \n",
    "model = get_model(img_size=img_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration du modèle\n",
    "\n",
    "Pour commencer, nous allons explorer le modèle U-Net. Pour ce faire, nous allons utiliser la fonction `plot_model` de `keras`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage\n",
    "\n",
    "Pour l'apprentissage, nous allons utiliser l'ensemble d'entrainement. Avant cela, nous allons devoir définir un certain nombre d'hyperparamètres. L'optimiseur, la fonction de coût, le nombre d'époques, la taille du batch, etc.\n",
    "\n",
    "Pour l'optimiseur, nous allons utiliser l'optimiseur Adam. Cet optimiseur est celui généralement utilisé pour la plupart des réseaux de neurones.\n",
    "\n",
    "Pour la fonction de coût, il nous faut pénaliser les cas où la valeur continue prédite est très éloignée de la valeur binaire réelle. Pour cela, nous allons utiliser une fonction de coût `binary_crossentropy`. Ces deux choix sont à renseigner dans la méthode `compile` de notre modèle.\n",
    "\n",
    "Enfin, nous allons entrainer notre modèle en utilisant la méthode `fit` de notre modèle. Nous devons spécifier les données d'entrainement, le nombre d'époques, la taille du batch, ainsi que les données utilisées pour évaluer notre modèle sur des données inconnues.\n",
    "\n",
    "Afin également de suivre l'évolution de notre modèle, nous allons utiliser un `callback` qui nous permettra d'enregistrer les poids de notre meilleur modèle selon la performance sur l'ensemble de validation.\n",
    "\n",
    "Une fois tous ces paramètres renseignés, nous pouvons lancer l'apprentissage de notre modèle. Attention, selon la configuration de votre machine, celui ci peut durer jusqu'à 30 minutes pour 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit # pour mesurer le temps d'éxecution de la cellule\n",
    "\n",
    "optimizer = ...\n",
    "loss = ...\n",
    "\n",
    "import tensorflow as tf\n",
    "model.compile(...) # a completer. De quoi à encore besoin le modèle ? \n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"emps_coria.keras\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "\n",
    "# LA fonction d'entrainement. Quelles sont les paramètres nécessaires pour entrainer le modèle ?\n",
    "# Documentation sur la fonction fit : https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "\n",
    "history = model.fit(... , callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats\n",
    "\n",
    "Pour analyser si tout s'est bien passé, nous allons observer les courbes d'apprentissage. Dans un premier temps, nous allons afficher la courbe de la fonction de coût sur l'ensemble d'apprentissage en fonction du nombre d'epochs. Puis, nous allons afficher la courbe de la fonction de coût sur l'ensemble de validation en fonction du nombre d'epochs.\n",
    "\n",
    "Pour cela, explorer la variable `history` de l'objet retourné par la méthode `fit`. Utilisez la librairie `matplotlib` pour afficher les courbes via la fonction `plot`.\n",
    "\n",
    "Le phénomène d'overfitting apparait lorsque la courbe de la fonction de coût sur l'ensemble de validation commence à augmenter alors que la courbe de la fonction de coût sur l'ensemble d'apprentissage continue de diminuer. Quel est alors le modèle optimal ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1,len(history.history[\"loss\"])+1)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(..., \"bo\", label=\"Training loss\")\n",
    "plt.plot(..., \"g\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle\n",
    "\n",
    "Afin d'évaluer notre modèle, nous allons d'abord sélectionner le modèle qui a obtenu les meilleures performances sur l'ensemble de validation.\n",
    "\n",
    "Ensuite, nous allons utiliser l'ensemble de test. Pour cela, nous allons devoir prédire les masques de segmentation pour quelques images de l'ensemble de test. Ensuite, nous allons comparer les masques prédits avec les masques réels. Pour cela, nous allons utiliser différentes métriques d'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# comment retrouver la \"meilleure\" epoch ? \n",
    "print(f\"Le meilleur modèle est atteint pour l'epoch {... }\")\n",
    "# chargement du meilleur modele  \n",
    "# model = keras.models.load_model(\"emps_coria.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation qualitative\n",
    "\n",
    "Pour l'évaluation qualitative, nous allons afficher 9 images de l'ensemble de test, avec les masques prédits correspondants. \n",
    "\n",
    "Pour cela, nous allons utiliser la librairie `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prend 9 images au hasard parmi la val pour affichage \n",
    "import numpy as np\n",
    "indexes = np.random.randint(0,len(val_input_imgs), 9)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import array_to_img\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # image à tester\n",
    "    test_img = val_input_imgs[indexes[i]]\n",
    "    # affichage de l'image\n",
    "    ax.imshow(...)\n",
    "    ax.axis(\"off\")\n",
    "    # calcul du masque prédit\n",
    "    pred = ... \n",
    "    #conversion en image\n",
    "    mask = array_to_img(pred)\n",
    "    ax.imshow(mask, alpha=0.25, cmap=\"viridis\") #alpha pour voir à travers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation quantitative\n",
    "\n",
    "Pour évaluer la qualité de notre modèle, nous allons utiliser différentes métriques d'évaluation. À la différence de l'évaluation qualitative effectuée précédemment, l'évaluation quantitative permet de mesurer la performance du modèle de manière plus précise. Pour cela, nous pouvons définir ce qui correspond à une bonne segmentation. \n",
    "\n",
    "La première manière, la plus simpliste, est de calculer la précision de classification en comparant les masques prédits avec les masques réels pixel par pixel. Si les deux masques sont identiques, alors la prédiction est parfaite. Cependant, cette méthode n'est pas très robuste. En effet, si un pixel est décalé, alors la prédiction sera considérée comme fausse. \n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}_{\\hat{y}_i = y_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Une deuxième manière de mesurer la performance est d'utiliser l'indice de Jaccard, appelé également IOU pour Intersection Over Union. Comme son nom l'indique, cette mesure est définie comme le rapport entre l'intersection et l'union du masque prédit et du masque correspondant à la vérité terrain. Plus l'indice de Jaccard est proche de 1, plus la prédiction est bonne.\n",
    "\n",
    "$$\n",
    "IOU = \\frac{|X \\cap Y|}{|X \\cup Y|}\n",
    "$$\n",
    "\n",
    "Enfin, une troisième mesure est le Dice Score. Cette mesure, similaire à l'indice de Jaccard,  est également basée sur l'intersection et l'union des masques prédits et réels. Plus le Dice Score est proche de 1, plus la prédiction est bonne. Le Dice Score est défini comme suit :\n",
    "\n",
    "$$\n",
    "Dice = \\frac{2 \\times |X \\cap Y|}{|X| + |Y|}\n",
    "$$\n",
    "\n",
    "Afin de mettre en oeuvre ces indices, il faut d'abord déterminer, pour chaque pixel, si il appartient au premier plan ou à l'arrière plan. Pour cela, nous allons utiliser un seuil. Si la probabilité prédite est supérieure à ce seuil, alors le pixel appartient au premier plan, sinon il appartient à l'arrière plan. Communément, le seuil est fixé à 0,5.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation du modele\n",
    "# intersection over union\n",
    "def iou_score(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou\n",
    "\n",
    "# dice score\n",
    "def dice_score(y_true, y_pred):\n",
    "    ... \n",
    "\n",
    "    return \n",
    "\n",
    "# accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# evaluation du modèle\n",
    "preds = ... # calcul des prédictions sur l'ensemble des images \n",
    "iou = []\n",
    "dice = []\n",
    "acc = []\n",
    "\n",
    "for i in range(len(val_input_imgs)):\n",
    "    # Comment comparer la vérité terrain avec les prédictions ? \n",
    "    iou.append(iou_score(...))\n",
    "    dice.append(dice_score(...))\n",
    "    acc.append(accuracy(...))\n",
    "\n",
    "# affichage des scores moyens \n",
    "print(f\"iou: {np.mean(iou)}\")\n",
    "print(f\"dice: {np.mean(dice)}\")\n",
    "print(f\"accuracy: {np.mean(acc)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuto_seg_chollet",
   "language": "python",
   "name": "tuto_seg_chollet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
